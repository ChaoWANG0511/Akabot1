import argparse
import random
import string
import os
import numpy as np
import console
from data import Data
from Model import creat_model
from unet import apply_unet
import conversion

class AcapellaBot:
    # å®šä¹‰model
    def __init__(self):

        m = apply_unet()
        console.log("Model has", m.count_params(), "params")

        m.compile(loss='mean_squared_error', optimizer='adam')

        self.model = m
        # need to know so that we can avoid rounding errors with spectrogram
        # this should represent how much the input gets downscaled
        # in the middle of the network
        self.peakDownscaleFactor = 4

    # è®­ç»ƒéªŒè¯model.fit, ç»™data, epochs, batch, æ˜¯å¦å­˜æƒé‡
    def train(self, data, epochs, batch=1):
        xTrain, yTrain = data.train()
        xValid, yValid = data.valid()

        console.log("Training for", epochs, "epochs on", len(xTrain), "examples")   # æ‰“å°

        self.model.fit(xTrain, yTrain, batch_size=batch, epochs=epochs, validation_data=(xValid, yValid))

        console.notify(str(epochs) + " Epochs Complete!", "Training on", data.inPath, "with size", batch)

        weightPath = ''.join(random.choice(string.digits) for _ in range(16)) + ".h5"
        console.log("Saving intermediate weights to", weightPath)
        self.saveWeights(weightPath)


    def saveWeights(self, path):
        self.model.save_weights(path, overwrite=True)
    def loadWeights(self, path):
        self.model.load_weights(path)

    # é¢„æµ‹modelï¼Œ æ•°æ®å¤„ç†stft
    def isolateVocals(self, path, fftWindowSize, phaseIterations=10):
        # audio stft å¾—å¹…å€¼
        console.log("Attempting to isolate vocals from", path)
        audio, sampleRate = conversion.loadAudioFile(path)   # éŸ³é¢‘çš„ä¿¡å·å€¼float ndarray
        spectrogram, phase = conversion.audioFileToSpectrogram(audio, fftWindowSize=fftWindowSize)     # stftå¾—åˆ°çš„çŸ©é˜µçš„å¹…å€¼å’Œç›¸ä½
        console.log("Retrieved spectrogram; processing...")

        # å¹…å€¼ è¿›æ¨¡åž‹ å¾—å¹…å€¼
        # newSpectrogram = self.model.predict(conversion.expandToGrid(spectrogram, self.peakDownscaleFactor)[np.newaxis, :, :, np.newaxis])[0][:spectrogram.shape[0], :spectrogram.shape[1]]
        expandedSpectrogram = conversion.expandToGrid(spectrogram, self.peakDownscaleFactor)   # å¹…å€¼æ”¾å¤§ä¸€äº›byåŠ 0ï¼Œä¸ºäº†èƒ½æ•´é™¤peakDownscaleFactor
        expandedSpectrogramWithBatchAndChannels = expandedSpectrogram[np.newaxis, :, :, np.newaxis]  # ç¬¬ä¸€ç»´åŠ batchï¼Œç¬¬å››ç»´åŠ channel
        predictedSpectrogramWithBatchAndChannels = self.model.predict(expandedSpectrogramWithBatchAndChannels)   # æ”¾å…¥modelåšé¢„æµ‹
        predictedSpectrogram = predictedSpectrogramWithBatchAndChannels[0, :, :, 0] # o /// o   # å–é¢„æµ‹ç»“æžœçš„ç¬¬0batchï¼Œç¬¬0channel,å³å†å˜å›žå¹…å€¼
        newSpectrogram = predictedSpectrogram[:spectrogram.shape[0], :spectrogram.shape[1]]    # å¹…å€¼ç¼©å°ä¸ºåŽŸæ¥çš„å°ºå¯¸
        console.log("Processed spectrogram; reconverting to audio")

        # å¹…å€¼åstftè½¬ä¸ºé˜¿å¡è´æ‹‰audioæ•°
        newAudio = conversion.spectrogramToAudioFile(newSpectrogram, fftWindowSize=fftWindowSize, phaseIterations=phaseIterations)   # phase?
        pathParts = os.path.split(path)
        fileNameParts = os.path.splitext(pathParts[1])
        outputFileNameBase = os.path.join(pathParts[0], fileNameParts[0] + "_acapella")
        console.log("Converted to audio; writing to", outputFileNameBase)

        # æ•°è½¬éŸ³ï¼Œå­˜
        conversion.saveAudioFile(newAudio, outputFileNameBase + ".wav", sampleRate)
        conversion.saveSpectrogram(newSpectrogram, outputFileNameBase + ".png")
        conversion.saveSpectrogram(spectrogram, os.path.join(pathParts[0], fileNameParts[0]) + ".png")
        console.log("Vocal isolation complete ðŸ‘Œ")


if __name__ == "__main__":
    # if data folder is specified, create a new data object and train on the data
    # if input audio is specified, infer on the input
    parser = argparse.ArgumentParser(description="Acapella extraction with unet")
    parser.add_argument("--fft", default=1536, type=int, help="Size of FFT windows")
    parser.add_argument("--data", default=None, type=str, help="Path containing training data")
    parser.add_argument("--split", default=0.9, type=float, help="Proportion of the data to train on")
    parser.add_argument("--epochs", default=3, type=int, help="Number of epochs to train.")
    parser.add_argument("--weights", default="weights.h5", type=str, help="h5 file to read/write weights to")
    parser.add_argument("--batch", default=1, type=int, help="Batch size for training")
    parser.add_argument("--phase", default=10, type=int, help="Phase iterations for reconstruction")
    parser.add_argument("--ratio_overlap", default=0.2, help="set overlap ratio for training slices")
    parser.add_argument("--time_scale", default=256, help="set a time scale to cut the spectrogram into slices")
    parser.add_argument("--load", action='store_true', help="Load previous weights file before starting")

    parser.add_argument("files", nargs="*", default=[])

    args = parser.parse_args()

    acapellabot = AcapellaBot()

    # ç»™data: è®­ç»ƒ
    if len(args.files) == 0 and args.data:
        console.log("No files provided; attempting to train on " + args.data + "...")
        if args.load:
            console.h1("Loading Weights")
            acapellabot.loadWeights(args.weights)
        console.h1("Loading Data")
        data = Data(args.data, args.fft, args.split)   # è®­ç»ƒç”¨çš„dataè§data.pyä¸­çš„ç±»Data
        console.h1("Training Model")
        acapellabot.train(data, args.epochs, args.batch)
        acapellabot.saveWeights(args.weights)

    # ç»™files: é¢„æµ‹
    elif len(args.files) > 0:
        console.log("Weights provided; performing inference on " + str(args.files) + "...")
        console.h1("Loading weights")
        acapellabot.loadWeights(args.weights)
        for f in args.files:
            acapellabot.isolateVocals(f, args.fft, args.phase)

    else:
        console.error("Please provide data to train on (--data) or files to infer on")