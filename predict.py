
import console
import conversion
import os
from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, UpSampling2D, Concatenate



def isolateVocals(self, path, fftWindowSize, phaseIterations=10):
    # audio stft å¾—å¹…å€¼
    console.log("Attempting to isolate vocals from", path)
    audio, sampleRate = conversion.loadAudioFile(path)   # éŸ³é¢‘çš„ä¿¡å·å€¼float ndarray
    spectrogram, phase = conversion.audioFileToSpectrogram(audio, fftWindowSize=fftWindowSize)     # stftå¾—åˆ°çš„çŸ©é˜µçš„å¹…å€¼å’Œç›¸ä½
    console.log("Retrieved spectrogram; processing...")

    # å¹…å€¼ è¿›æ¨¡å‹ å¾—å¹…å€¼
    # newSpectrogram = self.model.predict(conversion.expandToGrid(spectrogram, self.peakDownscaleFactor)[np.newaxis, :, :, np.newaxis])[0][:spectrogram.shape[0], :spectrogram.shape[1]]
    expandedSpectrogram = conversion.expandToGrid(spectrogram, self.peakDownscaleFactor)   # å¹…å€¼æ”¾å¤§ä¸€äº›byåŠ 0ï¼Œä¸ºäº†èƒ½æ•´é™¤peakDownscaleFactor
    expandedSpectrogramWithBatchAndChannels = expandedSpectrogram[np.newaxis, :, :, np.newaxis]  # ç¬¬ä¸€ç»´åŠ batchï¼Œç¬¬å››ç»´åŠ channel
    predictedSpectrogramWithBatchAndChannels = self.model.predict(expandedSpectrogramWithBatchAndChannels)   # æ”¾å…¥modelåšé¢„æµ‹
    predictedSpectrogram = predictedSpectrogramWithBatchAndChannels[0, :, :, 0] # o /// o   # å–é¢„æµ‹ç»“æœçš„ç¬¬0batchï¼Œç¬¬0channel,å³å†å˜å›å¹…å€¼
    newSpectrogram = predictedSpectrogram[:spectrogram.shape[0], :spectrogram.shape[1]]    # å¹…å€¼ç¼©å°ä¸ºåŸæ¥çš„å°ºå¯¸
    console.log("Processed spectrogram; reconverting to audio")

    # å¹…å€¼åstftè½¬ä¸ºé˜¿å¡è´æ‹‰audioæ•°
    newAudio = conversion.spectrogramToAudioFile(newSpectrogram, fftWindowSize=fftWindowSize, phaseIterations=phaseIterations)   # phase?
    pathParts = os.path.split(path)
    fileNameParts = os.path.splitext(pathParts[1])
    outputFileNameBase = os.path.join(pathParts[0], fileNameParts[0] + "_acapella")
    console.log("Converted to audio; writing to", outputFileNameBase)

    # æ•°è½¬éŸ³ï¼Œå­˜
    conversion.saveAudioFile(newAudio, outputFileNameBase + ".wav", sampleRate)
    conversion.saveSpectrogram(newSpectrogram, outputFileNameBase + ".png")
    conversion.saveSpectrogram(spectrogram, os.path.join(pathParts[0], fileNameParts[0]) + ".png")
    console.log("Vocal isolation complete ğŸ‘Œ")

def trainModel(epochs=6, batch=8):
    mashup = Input(shape=(None, None, 1), name='input')  # shapeä¸å«batch size, Noneæ„å‘³ç€å¯ä»¥éšä¾¿å–
    convA = Conv2D(64, 3, activation='relu', padding='same')(mashup)  # 64ä¸ªfilter, æ¯ä¸ªéƒ½æ˜¯3*3, è¾“å…¥paddingåŠ 0è‡³è¾“å‡ºå¤§å°ç­‰äºè¾“å…¥
    conv = Conv2D(64, 4, strides=2, activation='relu', padding='same', use_bias=False)(convA)  # é»˜è®¤True, ä½†è¿™å±‚ä¸ç”¨bias vector
    conv = BatchNormalization()(conv)

    convB = Conv2D(64, 3, activation='relu', padding='same')(conv)
    conv = Conv2D(64, 4, strides=2, activation='relu', padding='same', use_bias=False)(convB)
    conv = BatchNormalization()(conv)  # ä¸æ”¹å˜å°ºå¯¸

    conv = Conv2D(128, 3, activation='relu', padding='same')(conv)
    conv = Conv2D(128, 3, activation='relu', padding='same', use_bias=False)(conv)
    conv = BatchNormalization()(conv)
    conv = UpSampling2D((2, 2))(conv)  # è¡Œé‡å¤2æ¬¡ï¼Œåˆ—é‡å¤2æ¬¡ã€‚é»˜è®¤channels_last (default)ï¼š(batch, height, width, channels)   ä¸ºä»€ä¹ˆå¤§å°ï¼Ÿ

    conv = Concatenate()([conv, convB])  # é»˜è®¤æ²¿axis=-1
    conv = Conv2D(64, 3, activation='relu', padding='same')(conv)
    conv = Conv2D(64, 3, activation='relu', padding='same', use_bias=False)(conv)
    conv = BatchNormalization()(conv)
    conv = UpSampling2D((2, 2))(conv)

    conv = Concatenate()([conv, convA])
    conv = Conv2D(64, 3, activation='relu', padding='same')(conv)
    conv = Conv2D(64, 3, activation='relu', padding='same')(conv)

    conv = Conv2D(32, 3, activation='relu', padding='same')(conv)
    conv = Conv2D(1, 3, activation='relu', padding='same')(conv)  # è¾“å‡ºåª1channel
    acapella = conv

    m = Model(inputs=mashup, outputs=acapella)

    m.compile(loss='mean_squared_error', optimizer='adam')

    model = m

    xTrain, yTrain = train()
    xValid, yValid = valid()

    model.fit(xTrain, yTrain, batch_size=batch, epochs=epochs, validation_data=(xValid, yValid))
    weightPath = 'weights' + ".h5"
    model.save_weights(weightPath, overwrite=True)


def train(x, y, trainingSplit=0.9):
    return (x[:int(len(x) * trainingSplit)], y[:int(len(y) * trainingSplit)])


def valid(x, y, trainingSplit=0.9):
    return (x[int(len(x) * trainingSplit):], y[int(len(y) * trainingSplit):])


console.log("Weights provided; performing inference on " + str(args.files) + "...")
console.h1("Loading weights")
acapellabot.loadWeights(args.weights)
for f in args.files:
    acapellabot.isolateVocals(f, args.fft, args.phase)
