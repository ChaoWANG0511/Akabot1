# -*- coding: utf-8 -*-
"""musdb Unetplusplus.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T9rH4r5BrLrnrMfyyhLipBxStzlgntM5

# 数据预处理
"""

from IPython.display import Audio, display
import urllib.request
import numpy as np
import scipy.stats
import seaborn as sns
import pandas as pd
from urllib.request import urlopen
import matplotlib.pyplot as plt
from matplotlib import gridspec
from matplotlib.transforms import BlendedGenericTransform
import scikit_posthocs as sp
from math import ceil
from scipy.signal import stft, istft
import warnings
import csv
warnings.simplefilter(action='ignore', category=FutureWarning)

import musdb
mus = musdb.DB(download=True, subsets='train')

def chop(spectrogram, wanted_time, ratio_overlap):
  # 条件： wanted_time * ratio_overlap 是整数
  (I, F, T) = spectrogram.shape
  K = int(ceil((T / wanted_time - ratio_overlap)/(1 - ratio_overlap)))  # ceil取上,要分成多少块
  expanded_T = int(wanted_time + wanted_time * (1 - ratio_overlap) * (K - 1)) # 扩展后的time维度
  newSpectrogram = np.zeros((I, F, expanded_T))   # 右：多一些0，至能被girdsize整除
  newSpectrogram[:, :, :T] = spectrogram
  slices = np.empty((0, I, F, wanted_time), float)
  for k in range(0, K):   # 列
    s = newSpectrogram[:, :, round(k * wanted_time * (1-ratio_overlap)):round(k * wanted_time * (1-ratio_overlap) + wanted_time)] # 切为很多小块
    s = np.expand_dims(s, axis=0)
    slices = np.append(slices, s, axis=0)
  slices = np.transpose(slices, (0, 1, 3, 2)) # 想要[batch, channel, time, freaquency] 
  return slices

def dataset(nbmus, time=256, ratio_overlap=0.5):
  eps = np.finfo(np.float).eps  # small epsilon to avoid dividing by zero

  X = np.empty((0, 2, time, 2049), float)
  M = {'vocals': np.empty((0, 2, time, 2049), float),
      'drums': np.empty((0, 2, time, 2049), float),
      'bass': np.empty((0, 2, time, 2049), float),
      'other': np.empty((0, 2, time, 2049), float)} # [batch, channel, time, freaquency] 

  for track in nbmus:
    # print(track.name)
    x = np.abs(stft(track.audio.T, nperseg=4096, noverlap=3072)[-1])   # shape: (nb_channels=channel, nb_features=freq, nb_frames=time)
    xs = chop(x,time,ratio_overlap)
    X = np.append(X, xs, axis=0)
    print(track)

    P = {} # sources spectrograms
    # compute model as the sum of spectrograms 分母
    model = eps
    for name, source in track.sources.items():  # 遍历所有声部，求mask中的分母
      # compute spectrogram of target source:
      P[name] = np.abs(stft(source.audio.T, nperseg=4096, noverlap=3072)[-1])
      model += P[name]

    for name, source in track.sources.items(): # 遍历所有声部，用mask分离出各个声部
      # compute soft mask as the ratio between source spectrogram and total
      mask = P[name] / model
      masks = chop(mask,time,ratio_overlap)
      M[name] = np.append(M[name], masks, axis=0)

  return X, M

X, M = dataset(mus[0:9])


print(X.shape, M['vocals'].shape)

"""# Unet++"""

import keras
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input, Conv2D, ZeroPadding2D, UpSampling2D, Dense, concatenate, Conv2DTranspose  # merge, 
from tensorflow.keras.layers import MaxPool2D  #, GlobalAveragePooling2D
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Lambda
from tensorflow.keras.layers import ELU, LeakyReLU
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import GaussianDropout
from tensorflow import slice

import numpy as np

smooth = 1.
dropout_rate = 0.5
act = "relu"

########################################
# 2D Standard
########################################

def standard_unit(input_tensor, stage, nb_filter, kernel_size=3):

    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_1', kernel_initializer = 'he_normal', padding='same', data_format='channels_first', kernel_regularizer=l2(1e-4))(input_tensor)
    x = Dropout(dropout_rate, name='dp'+stage+'_1')(x)
    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_2', kernel_initializer = 'he_normal', padding='same', data_format='channels_first', kernel_regularizer=l2(1e-4))(x)
    x = Dropout(dropout_rate, name='dp'+stage+'_2')(x)

    return x

########################################

"""
Standard UNet++ [Zhou et.al, 2018]
Total params: 9,041,601
"""
def Nest_Net(img_rows=256, img_cols=2049, color_type=2, num_class=2, deep_supervision=False):

    nb_filter = [32,64,128,256,512]

    # Handle Dimension Ordering for different backends
    #global bn_axis
    bn_axis = 1
    img_input = Input(shape=(color_type, img_rows, img_cols), name='main_input')
    input_2048 = slice(img_input, [0, 0, 0, 0], [-1, -1, -1, 2048])

    conv1_1 = standard_unit(input_2048, stage='11', nb_filter=nb_filter[0])  # 大小不变：96，32 filters
    pool1 = MaxPool2D((2, 2), strides=(2, 2), name='pool1', data_format='channels_first')(conv1_1)  # 大小：96变48

    conv2_1 = standard_unit(pool1, stage='21', nb_filter=nb_filter[1])   # 大小不变：48，64 filters
    pool2 = MaxPool2D((2, 2), strides=(2, 2), name='pool2', data_format='channels_first')(conv2_1)   # 大小：48变24

    up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same', data_format='channels_first')(conv2_1)  #
    conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)  #
    conv1_2 = standard_unit(conv1_2, stage='12', nb_filter=nb_filter[0])   #

    conv3_1 = standard_unit(pool2, stage='31', nb_filter=nb_filter[2])  #
    pool3 = MaxPool2D((2, 2), strides=(2, 2), name='pool3', data_format='channels_first')(conv3_1)  #

    up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same', data_format='channels_first')(conv3_1)  #
    conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)    #
    conv2_2 = standard_unit(conv2_2, stage='22', nb_filter=nb_filter[1])  #

    up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same', data_format='channels_first')(conv2_2)   #
    conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)  #
    conv1_3 = standard_unit(conv1_3, stage='13', nb_filter=nb_filter[0])   #

    conv4_1 = standard_unit(pool3, stage='41', nb_filter=nb_filter[3])  #
    pool4 = MaxPool2D((2, 2), strides=(2, 2), name='pool4', data_format='channels_first')(conv4_1)   #

    up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same', data_format='channels_first')(conv4_1)  #
    conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)   #
    conv3_2 = standard_unit(conv3_2, stage='32', nb_filter=nb_filter[2])   #

    up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same', data_format='channels_first')(conv3_2)   #
    conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)  #
    conv2_3 = standard_unit(conv2_3, stage='23', nb_filter=nb_filter[1])  #

    up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same', data_format='channels_first')(conv2_3)   #
    conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)  #
    conv1_4 = standard_unit(conv1_4, stage='14', nb_filter=nb_filter[0])  #

    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])   #

    up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same', data_format='channels_first')(conv5_1)   #
    conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)  #
    conv4_2 = standard_unit(conv4_2, stage='42', nb_filter=nb_filter[3])   #

    up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same', data_format='channels_first')(conv4_2)   #
    conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)  #
    conv3_3 = standard_unit(conv3_3, stage='33', nb_filter=nb_filter[2])   #

    up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same', data_format='channels_first')(conv3_3)   #
    conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)    #
    conv2_4 = standard_unit(conv2_4, stage='24', nb_filter=nb_filter[1])    #

    up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same', data_format='channels_first')(conv2_4)   #
    conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)   #
    conv1_5 = standard_unit(conv1_5, stage='15', nb_filter=nb_filter[0])  #

    nestnet_output_1 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4), data_format='channels_first')(conv1_2)
    nestnet_output_2 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4), data_format='channels_first')(conv1_3)
    nestnet_output_3 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4), data_format='channels_first')(conv1_4)
    nestnet_output_4 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4), data_format='channels_first')(conv1_5)

    paddings = tf.constant([[0,0],[0,0],[0,0],[0,1]])
    output = tf.pad(nestnet_output_4, paddings, "CONSTANT")

    model = Model(inputs=img_input, outputs=output)

    return model

model = Nest_Net()

tf.keras.utils.plot_model(model, to_file='blstm.png', show_shapes=True)

"""# 训练"""

m_unet = Nest_Net()
from tensorflow.keras.metrics import Accuracy
m_unet.compile(loss='mean_squared_error', optimizer='adam', metrics=[Accuracy()])
m_unet.fit(X[:20,:,:,:], M['vocals'][:20,:,:,:], batch_size=2, epochs=20)

"""# 预测"""

track = [mus[-1]]

X, M = dataset(track)

X_origin = stft(track[0].audio.T, nperseg=4096, noverlap=3072)[-1]

print(X.shape, len(M), M['vocals'].shape, X_origin.shape)

M_predict = m_unet.predict(X)
print(M_predict.shape)

MM_predict = {'vocals': M_predict,
      'drums': M_predict,
      'bass': M_predict,
      'other':M_predict}

def ichop(X_origin, M, time_scale=256, ratio_overlap=0.5): # 输入只一首歌
  channel, frequency, time = X_origin.shape
  newM = {'vocals': np.empty((channel, frequency, time+time_scale), float),
      'drums': np.empty((channel, frequency, time+time_scale), float),
      'bass': np.empty((channel, frequency, time+time_scale), float),
      'other': np.empty((channel, frequency, time+time_scale), float)} # [channel, frequency, time] 
  for name, source in M.items():
    for i in range(source.shape[0]): # 遍历batch
      toInput = np.transpose(source[i,:,:,:], (0, 2, 1))  # (2, 2049, 240)
      if i == 0:
        newM[name][:,:, i:i + time_scale] = toInput
      else:
        
        newM[name][:,:, int(i * time_scale * (1 - ratio_overlap)):int((i * time_scale * (1 - ratio_overlap) + time_scale * ratio_overlap))] = np.true_divide(
              np.add(
              newM[name][:,:, int(i * time_scale * (1 - ratio_overlap)):int(i * time_scale * (1 - ratio_overlap) + time_scale * ratio_overlap)],toInput[:,:, :int(time_scale * ratio_overlap)]), 
              2)
        newM[name][:,:, int(i * time_scale * (1 - ratio_overlap) + time_scale * ratio_overlap):int(
              i * time_scale * (1 - ratio_overlap) + time_scale)] = toInput[:,:,-int(time_scale * (1 - ratio_overlap)):]
    newM[name] = newM[name][:channel, :frequency, :time]
  return newM



newM = ichop(X_origin, MM_predict)
print(newM['vocals'].shape)

def estimateSpectro(X_origin, newM):
  
  # small epsilon to avoid dividing by zero
  eps = np.finfo(np.float).eps
  # compute model as the sum of spectrograms
  model = eps

  for name, source in newM.items():  # 遍历所有声部，求mask中的分母
    model += newM[name]


  # now performs separation
  estimates = {}
  for name, source in newM.items(): # 遍历所有声部，用mask分离出各个声部
    # compute soft mask as the ratio between source spectrogram and total
    Mask = newM[name] / model

    # multiply the mix by the mask
    Yj = Mask * X_origin

    # invert to time domain
    target_estimate = istft(Yj, nperseg=4096, noverlap=3072)[1].T

    # set this as the source estimate
    estimates[name] = target_estimate

  return estimates

estimates = estimateSpectro(X_origin, newM)

from IPython.display import Audio, display

for target, estimate in estimates.items():
  display(Audio(estimate.T, rate=track[0].rate))

import museval

track_scores = museval.eval_mus_track(track[0], estimates)
print(track_scores)