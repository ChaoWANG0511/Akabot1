{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaow\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\chaow\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\chaow\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\chaow\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\chaow\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\chaow\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from math import ceil\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 mixtures in the given path to train\n",
      "{\"055 - Angels In Amplifiers - I'm Alright\": [\"DSD100subset/Mixtures/Dev\\\\055 - Angels In Amplifiers - I'm Alright/mixture.wav\", \"DSD100subset/Sources/Dev\\\\055 - Angels In Amplifiers - I'm Alright/bass.wav\", \"DSD100subset/Sources/Dev\\\\055 - Angels In Amplifiers - I'm Alright/drums.wav\", \"DSD100subset/Sources/Dev\\\\055 - Angels In Amplifiers - I'm Alright/other.wav\", \"DSD100subset/Sources/Dev\\\\055 - Angels In Amplifiers - I'm Alright/vocals.wav\"], '081 - Patrick Talbot - Set Me Free': ['DSD100subset/Mixtures/Dev\\\\081 - Patrick Talbot - Set Me Free/mixture.wav', 'DSD100subset/Sources/Dev\\\\081 - Patrick Talbot - Set Me Free/bass.wav', 'DSD100subset/Sources/Dev\\\\081 - Patrick Talbot - Set Me Free/drums.wav', 'DSD100subset/Sources/Dev\\\\081 - Patrick Talbot - Set Me Free/other.wav', 'DSD100subset/Sources/Dev\\\\081 - Patrick Talbot - Set Me Free/vocals.wav']}\n"
     ]
    }
   ],
   "source": [
    "def listdirInMac(path):\n",
    "    os_list = os.listdir(path)\n",
    "    for item in os_list:\n",
    "        if item.startswith('.') and os.path.isfile(os.path.join(path, item)):\n",
    "            os_list.remove(item)\n",
    "    return os_list\n",
    "\n",
    "def traversalDir_FirstDir(path):\n",
    "    dict = {}\n",
    "    files = listdirInMac(path)\n",
    "    for file in files:\n",
    "        m = os.path.join(path, file)\n",
    "        h = os.path.split(m)\n",
    "        dict[h[1]] = []\n",
    "        song_wav = listdirInMac(m)\n",
    "        m = m + '/'\n",
    "        for track in song_wav:\n",
    "            value = os.path.join(m, track)\n",
    "            dict[h[1]].append(value)\n",
    "    return dict\n",
    "\n",
    "mix_path = traversalDir_FirstDir('DSD100subset/Mixtures/Dev')\n",
    "\n",
    "sou_path = traversalDir_FirstDir('DSD100subset/Sources/Dev')\n",
    "\n",
    "all_path = mix_path.copy()\n",
    "for key in all_path.keys():\n",
    "    all_path[key].extend(sou_path[key])\n",
    "\n",
    "print(\"%d mixtures in the given path to train\"%len(mix_path))\n",
    "print(all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAudioFile(filePath):\n",
    "    audio, sampleRate = librosa.load(filePath, sr = 44100)  # Load an audio file as a floating point time series， 默认采样率sr=22050\n",
    "    return audio, sampleRate\n",
    "\n",
    "# Return a 2d numpy array of the spectrogram\n",
    "def audioFileToSpectrogram(audioFile, fftWindowSize = 1024):\n",
    "    # time resolution N/fs=1024/44100=23ms\n",
    "    spectrogram = librosa.stft(audioFile, fftWindowSize)   # 返回复数值矩阵D , STFT矩阵D中的行数是（1 + 第二个参数一般2的幂n_fft / 2）\n",
    "    phase = np.angle(spectrogram)\n",
    "    amplitude = np.log1p(np.abs(spectrogram))   # np.abs(D[f, t]) is the magnitude of frequency bin f at frame帧 t， loge(1+ )\n",
    "    return amplitude, phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "频谱矩阵做random time stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# audio/spectrogram.py\n",
    "\n",
    "def time_stretch(\n",
    "        spectrograms,\n",
    "        factor=1.0,\n",
    "        method=tf.image.ResizeMethod.BILINEAR):\n",
    "    \"\"\" Time stretch a spectrogram preserving shape in tensorflow. Note that\n",
    "    this is an approximation in the frequency domain.\n",
    "    :param spectrogram: Input spectrogram to be time stretched as tensor.\n",
    "    :param factor: (Optional) Time stretch factor, must be >0, default to 1.\n",
    "    :param mehtod: (Optional) Interpolation method, default to BILINEAR.\n",
    "    :returns: Time stretched spectrogram as tensor with same shape.\n",
    "    \"\"\"\n",
    "    F = tf.shape(spectrograms)[0]\n",
    "    T = tf.shape(spectrograms)[1]\n",
    "    T_ts = tf.cast(tf.cast(T, tf.float32) * factor, tf.int32)[0]  #cast: 改数据类型\n",
    "    print(T, T_ts, F)  # 7587, 6097, 513\n",
    "    ts_spec = tf.image.resize(\n",
    "        spectrograms,\n",
    "        size = [F, T_ts],\n",
    "        method=method)      # 双线性插值\n",
    "    return tf.image.resize_with_crop_or_pad(ts_spec, F, T)   # 补/切回原来形状\n",
    "\n",
    "\n",
    "def random_time_stretch(spectrogram, factor_min=0.9, factor_max=1.1, **kwargs):\n",
    "    \"\"\" Time stretch a spectrogram preserving shape with random ratio in\n",
    "    tensorflow. Applies time_stretch to spectrogram with a random ratio drawn\n",
    "    uniformly in [factor_min, factor_max].\n",
    "    :param spectrogram: Input spectrogram to be time stretched as tensor.\n",
    "    :param factor_min: (Optional) Min time stretch factor, default to 0.9.\n",
    "    :param factor_max: (Optional) Max time stretch factor, default to 1.1.\n",
    "    :returns: Randomly time stretched spectrogram as tensor with same shape.\n",
    "    \"\"\"\n",
    "    factor = tf.random.uniform(\n",
    "        shape=(1,),\n",
    "        seed=0) * (factor_max - factor_min) + factor_min\n",
    "    return time_stretch(spectrogram, factor=factor, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "频谱矩阵做random pitch shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_shift(\n",
    "        spectrograms,\n",
    "        semitone_shift=0.0,\n",
    "        method=tf.image.ResizeMethod.BILINEAR):\n",
    "    \"\"\" Pitch shift a spectrogram preserving shape in tensorflow. Note that\n",
    "    this is an approximation in the frequency domain.\n",
    "    :param spectrogram: Input spectrogram to be pitch shifted as tensor.\n",
    "    :param semitone_shift: (Optional) Pitch shift in semitone半音, default to 0.0.\n",
    "    :param mehtod: (Optional) Interpolation method, default to BILINEAR.\n",
    "    :returns: Pitch shifted spectrogram (same shape as spectrogram).\n",
    "    \"\"\"\n",
    "    factor = 2 ** (semitone_shift / 12.)\n",
    "    F = tf.shape(spectrograms)[0]\n",
    "    T = tf.shape(spectrograms)[1]\n",
    "    F_ps = tf.cast(tf.cast(F, tf.float32) * factor, tf.int32)[0]\n",
    "    ps_spec = tf.image.resize(\n",
    "        spectrograms,\n",
    "        [F_ps,T],\n",
    "        method=method)\n",
    "    paddings = [[0, tf.maximum(0, F - F_ps)], [0, 0], [0, 0]]\n",
    "    return tf.pad(ps_spec[:F,:,:], paddings, 'CONSTANT')\n",
    "\n",
    "\n",
    "def random_pitch_shift(spectrogram, shift_min=-1., shift_max=1., **kwargs):\n",
    "    \"\"\" Pitch shift a spectrogram preserving shape with random ratio in\n",
    "    tensorflow. Applies pitch_shift to spectrogram with a random shift\n",
    "    amount (expressed in semitones) drawn uniformly in [shift_min, shift_max].\n",
    "    :param spectrogram: Input spectrogram to be pitch shifted as tensor.\n",
    "    :param shift_min: (Optional) Min pitch shift in semitone, default to -1.\n",
    "    :param shift_max: (Optional) Max pitch shift in semitone, default to 1.\n",
    "    :returns: Randomly pitch shifted spectrogram (same shape as spectrogram).\n",
    "    \"\"\"\n",
    "    semitone_shift = tf.random.uniform(\n",
    "        shape=(1,),\n",
    "        seed=0) * (shift_max - shift_min) + shift_min\n",
    "    return pitch_shift(spectrogram, semitone_shift=semitone_shift, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandToGrid(spectrogram, time_scale, ratio_overlap):\n",
    "    newY = int(ceil((spectrogram.shape[1] - time_scale)/(1-ratio_overlap)/time_scale) * (1-ratio_overlap)*time_scale)+time_scale  # ceil取上\n",
    "    #newX = int(spectrogram.shape[0]/64)*64\n",
    "    newX = 512\n",
    "    newSpectrogram = np.zeros((newX, newY))   # 右，下：多一些0，至能被girdsize整除\n",
    "    print(spectrogram.shape,\"original shape\")\n",
    "    print(newSpectrogram.shape,\"expanded shape\")\n",
    "    newSpectrogram[:512, :spectrogram.shape[1]] = spectrogram[:512,:]\n",
    "    return newSpectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop(matrix, time_scale, ratio_overlap):\n",
    "    slices = []\n",
    "    for time in range(0, ceil((matrix.shape[1]-time_scale)/time_scale/(1-ratio_overlap))+1):   # 列\n",
    "            s = matrix[: (int(matrix.shape[0]/64))*64,\n",
    "                       int(time * time_scale*(1-ratio_overlap)) :int(time * time_scale*(1-ratio_overlap))+time_scale ]\n",
    "            slices.append(s)\n",
    "    print(s.shape)\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample rate:  44100\n",
      "spectrogram shape: (513, 7587, 1)\n",
      "each_song shape:  (513, 7587, 5)\n",
      "tf.Tensor(7587, shape=(), dtype=int32) tf.Tensor(6981, shape=(), dtype=int32) tf.Tensor(513, shape=(), dtype=int32)\n",
      "Time stretch Spectrogram shape: (513, 7587, 5)\n",
      "Pitch swift Spectrogram shape: (513, 7587, 5)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "(513, 7587) original shape\n",
      "(512, 7616) expanded shape\n",
      "(512, 64)\n",
      "sample rate:  44100\n",
      "spectrogram shape: (513, 7244, 1)\n",
      "each_song shape:  (513, 7244, 5)\n",
      "tf.Tensor(7244, shape=(), dtype=int32) tf.Tensor(6734, shape=(), dtype=int32) tf.Tensor(513, shape=(), dtype=int32)\n",
      "Time stretch Spectrogram shape: (513, 7244, 5)\n",
      "Pitch swift Spectrogram shape: (513, 7244, 5)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n",
      "(513, 7244) original shape\n",
      "(512, 7264) expanded shape\n",
      "(512, 64)\n"
     ]
    }
   ],
   "source": [
    "def song2spectrogram(all_path_para, fftWindowSize=1024, time_scale=64, overlap_ratio=0.5):\n",
    "    \n",
    "\n",
    "    x = np.empty((0,512, 64), float)\n",
    "    x_phase = []\n",
    "    y_bass = np.empty((0,512, 64), float)\n",
    "    y_drums = np.empty((0,512, 64), float)\n",
    "    y_other = np.empty((0,512, 64), float)\n",
    "    y_vocals = np.empty((0,512, 64), float)\n",
    "    # 第0维：每首歌\n",
    "    # 第1维：height = f\n",
    "    # 第2维：width = t\n",
    "    # 第3维：每个音轨 - x,y_bass,y_drums,y_other,y_vocals 共5个\n",
    "    for key in all_path_para:   # 每首歌\n",
    "        path_list = all_path_para[key][:]\n",
    "        audio, sampleRate = loadAudioFile(path_list[0])\n",
    "        spectrogram, phase = audioFileToSpectrogram(audio, fftWindowSize)\n",
    "        each_song = np.empty([int(spectrogram.shape[0]),int(spectrogram.shape[1]),0])\n",
    "        for path in path_list:    # 每个音轨\n",
    "            audio, sampleRate = loadAudioFile(path)           \n",
    "            spectrogram, phase = audioFileToSpectrogram(audio, fftWindowSize)\n",
    "            spectrogram = spectrogram[:,:,np.newaxis]            \n",
    "            each_song = np.append(each_song, spectrogram, axis=-1)\n",
    "        print('sample rate: ',sampleRate)\n",
    "        print(\"spectrogram shape:\",spectrogram.shape)\n",
    "        print(\"each_song shape: \", each_song.shape)  # (5, 513, 7587)\n",
    "        \n",
    "        # 输入输出都做数据增强：time stretch，pitch swift\n",
    "        ts_spectrogram = random_time_stretch(each_song)   \n",
    "        print(\"Time stretch Spectrogram shape:\",ts_spectrogram.shape)\n",
    "        ps_spectrogram = random_pitch_shift(each_song)\n",
    "        print(\"Pitch swift Spectrogram shape:\",ps_spectrogram.shape)\n",
    "                \n",
    "        [hight, weidth, stem] = each_song.shape\n",
    "        for spect in [each_song, ts_spectrogram, ps_spectrogram]:\n",
    "            for i in range(stem):\n",
    "                expandedSpectrogram = expandToGrid(spect[:,:,i],time_scale,overlap_ratio)\n",
    "                slices = chop(expandedSpectrogram, time_scale,overlap_ratio)  \n",
    "                if i==0:\n",
    "                    x = np.append(x, slices,axis=0)\n",
    "                    x_phase.append(phase)\n",
    "                elif i==1:\n",
    "                    y_bass = np.append(y_bass, slices,axis=0)\n",
    "                elif i==2:\n",
    "                    y_drums = np.append(y_drums, slices,axis=0)\n",
    "                elif i==3:\n",
    "                    y_other = np.append(y_other, slices,axis=0)\n",
    "                else:   #i==4\n",
    "                    y_vocals = np.append(y_vocals, slices,axis=0)\n",
    "                \n",
    "    output = [x,x_phase,y_bass,y_drums,y_other,y_vocals]\n",
    "  \n",
    "    return output\n",
    "\n",
    "list_dataset = song2spectrogram(all_path_para=all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1389, 512, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dataset[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1389, 512, 64, 1) (1389, 512, 64, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "x = list_dataset[0]\n",
    "y_bass = list_dataset[2]\n",
    "y_drums = list_dataset[3]\n",
    "y_other = list_dataset[4]\n",
    "y_vocals = list_dataset[5]\n",
    "\n",
    "x = np.array(x)[:, :, :, np.newaxis]\n",
    "y_bass = np.array(y_bass)[:, :, :, np.newaxis]\n",
    "y_drums = np.array(y_drums)[:, :, :, np.newaxis]\n",
    "y_other = np.array(y_drums)[:, :, :, np.newaxis]\n",
    "y_vocals = np.array(y_drums)[:, :, :, np.newaxis]\n",
    "\n",
    "print(x.shape, y_vocals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Reshape, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from math import ceil\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    Dropout,\n",
    "    multiply,\n",
    "    ReLU)\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras.backend import int_shape, squeeze\n",
    "\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_unet():\n",
    "    \"\"\" Apply a convolutionnal U-net to model a single instrument (one U-net\n",
    "    is used for each instrument).\n",
    "    :param input_tensor:\n",
    "    :param output_name: (Optional) , default to 'output'\n",
    "    :param params: (Optional) , default to empty dict.\n",
    "    :param output_mask_logit: (Optional) , default to False.\n",
    "    \"\"\"\n",
    "    conv_n_filters = [16, 32, 64, 128, 256, 512]\n",
    "    conv_activation_layer = ReLU()\n",
    "    deconv_activation_layer = ReLU()\n",
    "    kernel_initializer = he_uniform(seed=50)\n",
    "    conv2d_factory = partial(\n",
    "        Conv2D,\n",
    "        strides=(2, 2),\n",
    "        padding='same',\n",
    "        kernel_initializer=kernel_initializer)\n",
    "\n",
    "    #input_tensor = Input(shape=(None, None, 1), name='input')\n",
    "    input_tensor = Input(shape=(512, 64, 1), name='input')\n",
    "\n",
    "    # First layer.\n",
    "    conv1 = conv2d_factory(conv_n_filters[0], (5, 5))(input_tensor)\n",
    "    batch1 = BatchNormalization(axis=-1)(conv1)\n",
    "    rel1 = conv_activation_layer(batch1)\n",
    "    # Second layer.\n",
    "    conv2 = conv2d_factory(conv_n_filters[1], (5, 5))(rel1)\n",
    "    batch2 = BatchNormalization(axis=-1)(conv2)\n",
    "    rel2 = conv_activation_layer(batch2)\n",
    "    # Third layer.\n",
    "    conv3 = conv2d_factory(conv_n_filters[2], (5, 5))(rel2)\n",
    "    batch3 = BatchNormalization(axis=-1)(conv3)\n",
    "    rel3 = conv_activation_layer(batch3)\n",
    "    # Fourth layer.\n",
    "    conv4 = conv2d_factory(conv_n_filters[3], (5, 5))(rel3)\n",
    "    batch4 = BatchNormalization(axis=-1)(conv4)\n",
    "    rel4 = conv_activation_layer(batch4)\n",
    "    # Fifth layer.\n",
    "    conv5 = conv2d_factory(conv_n_filters[4], (5, 5))(rel4)\n",
    "    batch5 = BatchNormalization(axis=-1)(conv5)\n",
    "    rel5 = conv_activation_layer(batch5)\n",
    "    # Sixth layer\n",
    "    conv6 = conv2d_factory(conv_n_filters[5], (5, 5))(rel5)\n",
    "    batch6 = BatchNormalization(axis=-1)(conv6)\n",
    "    _ = conv_activation_layer(batch6)\n",
    "    #\n",
    "    #\n",
    "    conv2d_transpose_factory = partial(\n",
    "        Conv2DTranspose,\n",
    "        strides=(2, 2),\n",
    "        padding='same',\n",
    "        kernel_initializer=kernel_initializer)\n",
    "    #\n",
    "    up1 = conv2d_transpose_factory(conv_n_filters[4], (5, 5))((conv6))\n",
    "    up1 = deconv_activation_layer(up1)\n",
    "    batch7 = BatchNormalization(axis=-1)(up1)\n",
    "    drop1 = Dropout(0.5)(batch7)\n",
    "    merge1 = Concatenate(axis=-1)([conv5, drop1])\n",
    "    #\n",
    "    up2 = conv2d_transpose_factory(conv_n_filters[3], (5, 5))((merge1))\n",
    "    up2 = deconv_activation_layer(up2)\n",
    "    batch8 = BatchNormalization(axis=-1)(up2)\n",
    "    drop2 = Dropout(0.5)(batch8)\n",
    "    merge2 = Concatenate(axis=-1)([conv4, drop2])\n",
    "    #\n",
    "    up3 = conv2d_transpose_factory(conv_n_filters[2], (5, 5))((merge2))\n",
    "    up3 = deconv_activation_layer(up3)\n",
    "    batch9 = BatchNormalization(axis=-1)(up3)\n",
    "    drop3 = Dropout(0.5)(batch9)\n",
    "    merge3 = Concatenate(axis=-1)([conv3, drop3])\n",
    "    #\n",
    "    up4 = conv2d_transpose_factory(conv_n_filters[1], (5, 5))((merge3))\n",
    "    up4 = deconv_activation_layer(up4)\n",
    "    batch10 = BatchNormalization(axis=-1)(up4)\n",
    "    merge4 = Concatenate(axis=-1)([conv2, batch10])\n",
    "    #\n",
    "    up5 = conv2d_transpose_factory(conv_n_filters[0], (5, 5))((merge4))\n",
    "    up5 = deconv_activation_layer(up5)\n",
    "    batch11 = BatchNormalization(axis=-1)(up5)\n",
    "    merge5 = Concatenate(axis=-1)([conv1, batch11])\n",
    "    #\n",
    "    up6 = conv2d_transpose_factory(1, (5, 5), strides=(2, 2))((merge5))\n",
    "    up6 = deconv_activation_layer(up6)\n",
    "    batch12 = BatchNormalization(axis=-1)(up6)\n",
    "    # Last layer to ensure initial shape reconstruction.\n",
    "    up7 = Conv2D(1, (4, 4), dilation_rate=(2, 2), activation='sigmoid', padding='same', kernel_initializer=kernel_initializer)((batch12))\n",
    "    output = multiply([up7, input_tensor])\n",
    "\n",
    "    m = Model(inputs=input_tensor, outputs=output)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_unet = apply_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_unet.compile(loss='mean_squared_error', optimizer='adam', metrics=[Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1250 samples, validate on 139 samples\n",
      "1250/1250 [==============================] - 86s 69ms/sample - loss: 0.0444 - accuracy: 0.0289 - val_loss: 0.0363 - val_accuracy: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a7a2369978>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_unet.fit(x,y , batch_size=50, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_unet(input_tensor, instruments, params={}):\n",
    "    \"\"\" Apply softmax to multitrack unet in order to have mask suming to one.\n",
    "    :param input_tensor: Tensor to apply blstm to.\n",
    "    :param instruments: Iterable that provides a collection of instruments.\n",
    "    :param params: (Optional) dict of BLSTM parameters.\n",
    "    :returns: Created output tensor dict.\n",
    "    \"\"\"\n",
    "    logit_mask_list = []\n",
    "    for instrument in instruments:\n",
    "        out_name = f'{instrument}_spectrogram'\n",
    "        logit_mask_list.append(\n",
    "            apply_unet(\n",
    "                input_tensor,\n",
    "                output_name=out_name,\n",
    "                params=params,\n",
    "                output_mask_logit=True))\n",
    "    masks = Softmax(axis=4)(tf.stack(logit_mask_list, axis=4))\n",
    "    output_dict = {}\n",
    "    for i, instrument in enumerate(instruments):\n",
    "        out_name = f'{instrument}_spectrogram'\n",
    "        output_dict[out_name] = Multiply(name=out_name)([\n",
    "            masks[..., i],\n",
    "            input_tensor])\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_estimator(params):\n",
    "    \"\"\" Creates estimator.\n",
    "    :param params: TF params to build estimator from.\n",
    "    :returns: Built estimator.\n",
    "    \"\"\"\n",
    "    session_config = tf.compat.v1.ConfigProto()\n",
    "    session_config.gpu_options.per_process_gpu_memory_fraction = 0.45\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        model_dir=params['model_dir'],\n",
    "        params=params,\n",
    "        config=tf.estimator.RunConfig(\n",
    "            save_checkpoints_steps=params['save_checkpoints_steps'],\n",
    "            tf_random_seed=params['random_seed'],\n",
    "            save_summary_steps=params['save_summary_steps'],\n",
    "            session_config=session_config,\n",
    "            log_step_count_steps=10,\n",
    "            keep_checkpoint_max=2))\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_train_spec(params, audio_adapter, audio_path):\n",
    "    \"\"\" Creates train spec.\n",
    "    :param params: TF params to build spec from.\n",
    "    :returns: Built train spec.\n",
    "    \"\"\"\n",
    "    input_fn = partial(get_training_dataset, params, audio_adapter, audio_path)\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=input_fn,\n",
    "        max_steps=params['train_max_steps'])\n",
    "    return train_spec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
