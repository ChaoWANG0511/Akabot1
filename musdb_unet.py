# -*- coding: utf-8 -*-
"""musdb Unet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y8eRDi32U4cz8buttfACj6l1gIIcMReO

# 数据预处理
"""

from IPython.display import Audio, display
import urllib.request
import numpy as np
import scipy.stats
import seaborn as sns
import pandas as pd
from urllib.request import urlopen
import matplotlib.pyplot as plt
from matplotlib import gridspec
from matplotlib.transforms import BlendedGenericTransform
import scikit_posthocs as sp
from math import ceil
from scipy.signal import stft, istft
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import musdb
mus = musdb.DB(download=True, subsets='train')

def chop(spectrogram, wanted_time, ratio_overlap):
  # 条件： wanted_time * ratio_overlap 是整数
  (I, F, T) = spectrogram.shape
  K = int(ceil((T / wanted_time - ratio_overlap)/(1 - ratio_overlap)))  # ceil取上,要分成多少块
  expanded_T = int(wanted_time + wanted_time * (1 - ratio_overlap) * (K - 1)) # 扩展后的time维度
  newSpectrogram = np.zeros((I, F, expanded_T))   # 右：多一些0，至能被girdsize整除
  newSpectrogram[:, :, :T] = spectrogram
  slices = np.empty((0, I, F, wanted_time), float)
  for k in range(0, K):   # 列
    s = newSpectrogram[:, :, round(k * wanted_time * (1-ratio_overlap)):round(k * wanted_time * (1-ratio_overlap) + wanted_time)] # 切为很多小块
    s = np.expand_dims(s, axis=0)
    slices = np.append(slices, s, axis=0)
  slices = np.transpose(slices, (0, 1, 3, 2)) # 想要[batch, channel, time, freaquency] 
  return slices

def dataset(nbmus, time=256, ratio_overlap=0.5):
  eps = np.finfo(np.float).eps  # small epsilon to avoid dividing by zero

  X = np.empty((0, 2, time, 2049), float)
  M = {'vocals': np.empty((0, 2, time, 2049), float),
      'drums': np.empty((0, 2, time, 2049), float),
      'bass': np.empty((0, 2, time, 2049), float),
      'other': np.empty((0, 2, time, 2049), float)} # [batch, channel, time, freaquency] 

  for track in nbmus:
    # print(track.name)
    x = np.abs(stft(track.audio.T, nperseg=4096, noverlap=3072)[-1])   # shape: (nb_channels=channel, nb_features=freq, nb_frames=time)
    xs = chop(x,time,ratio_overlap)
    X = np.append(X, xs, axis=0)
    print(track)

    P = {} # sources spectrograms
    # compute model as the sum of spectrograms 分母
    model = eps
    for name, source in track.sources.items():  # 遍历所有声部，求mask中的分母
      # compute spectrogram of target source:
      P[name] = np.abs(stft(source.audio.T, nperseg=4096, noverlap=3072)[-1])
      model += P[name]

    for name, source in track.sources.items(): # 遍历所有声部，用mask分离出各个声部
      # compute soft mask as the ratio between source spectrogram and total
      mask = P[name] / model
      masks = chop(mask,time,ratio_overlap)
      M[name] = np.append(M[name], masks, axis=0)

  return X, M

X, M = dataset(mus[0:9])

print(X.shape, M['vocals'].shape)

"""# Unet"""

import numpy as np
import os
from tensorflow.keras.layers import Input, Reshape, Lambda
from tensorflow.keras.models import Model
from math import ceil

from functools import partial

import tensorflow as tf
from functools import partial
from tensorflow.keras.layers import (
    BatchNormalization,
    Concatenate,
    Conv2D,
    Conv2DTranspose,
    Dropout,
    multiply,
    ReLU)

from tensorflow.keras.metrics import Accuracy
from tensorflow import slice

def apply_unet():
    """ Apply a convolutionnal U-net to model a single instrument (one U-net
    is used for each instrument).
    :param input_tensor:
    :param output_name: (Optional) , default to 'output'
    :param params: (Optional) , default to empty dict.
    :param output_mask_logit: (Optional) , default to False.
    """
    conv_n_filters = [16, 32, 64, 128, 256, 512]
    conv_activation_layer = ReLU()
    deconv_activation_layer = ReLU()
    kernel_initializer = tf.keras.initializers.he_uniform(seed=50)
    conv2d_factory = partial(
        Conv2D,
        strides=(2, 2),
        padding='same',
        data_format='channels_first',
        kernel_initializer=kernel_initializer)

    #input_tensor = Input(shape=(None, None, 1), name='input')
    input_tensor = Input(shape=(2, 256, 2049), name='input')   #(512, 64, 1)
    b = slice(input_tensor, [0, 0, 0, 0], [-1, -1, -1, 2048])

    # First layer.
    conv1 = conv2d_factory(conv_n_filters[0], (5, 5))(b)
    batch1 = BatchNormalization(axis=-3)(conv1)
    rel1 = ReLU()(batch1)
    # Second layer.
    conv2 = conv2d_factory(conv_n_filters[1], (5, 5))(rel1)
    batch2 = BatchNormalization(axis=-3)(conv2)
    rel2 = ReLU()(batch2)
    # Third layer.
    conv3 = conv2d_factory(conv_n_filters[2], (5, 5))(rel2)
    batch3 = BatchNormalization(axis=-3)(conv3)
    rel3 = ReLU()(batch3)
    # Fourth layer.
    conv4 = conv2d_factory(conv_n_filters[3], (5, 5))(rel3)
    batch4 = BatchNormalization(axis=-3)(conv4)
    rel4 = ReLU()(batch4)
    # Fifth layer.
    conv5 = conv2d_factory(conv_n_filters[4], (5, 5))(rel4)
    batch5 = BatchNormalization(axis=-3)(conv5)
    rel5 = ReLU()(batch5)
    # Sixth layer
    conv6 = conv2d_factory(conv_n_filters[5], (5, 5))(rel5)
    batch6 = BatchNormalization(axis=-3)(conv6)
    _ = ReLU()(batch6)
    #
  
    conv2d_transpose_factory = partial(
        Conv2DTranspose,
        strides=(2, 2),
        padding='same',
        data_format='channels_first',
        kernel_initializer=kernel_initializer)
    #
    up1 = conv2d_transpose_factory(conv_n_filters[4], (5, 5))((conv6))
    up1 = ReLU()(up1)
    batch7 = BatchNormalization(axis=-3)(up1)
    drop1 = Dropout(0.5)(batch7)
    
    merge1 = Concatenate(axis=-3)([conv5, drop1])
    #
    up2 = conv2d_transpose_factory(conv_n_filters[3], (5, 5))((merge1))
    up2 = ReLU()(up2)
    batch8 = BatchNormalization(axis=-3)(up2)
    drop2 = Dropout(0.5)(batch8)
    merge2 = Concatenate(axis=-3)([conv4, drop2])
    #
    up3 = conv2d_transpose_factory(conv_n_filters[2], (5, 5))((merge2))
    up3 = ReLU()(up3)
    batch9 = BatchNormalization(axis=-3)(up3)
    drop3 = Dropout(0.5)(batch9)
    merge3 = Concatenate(axis=-3)([conv3, drop3])
    #
    up4 = conv2d_transpose_factory(conv_n_filters[1], (5, 5))((merge3))
    up4 = ReLU()(up4)
    batch10 = BatchNormalization(axis=-3)(up4)
    merge4 = Concatenate(axis=-3)([conv2, batch10])
    #
    up5 = conv2d_transpose_factory(conv_n_filters[0], (5, 5))((merge4))
    up5 = ReLU()(up5)
    batch11 = BatchNormalization(axis=-3)(up5)
    merge5 = Concatenate(axis=-3)([conv1, batch11])
    #
    up6 = conv2d_transpose_factory(1, (5, 5), strides=(2, 2))((merge5))
    up6 = ReLU()(up6)
    batch12 = BatchNormalization(axis=-3)(up6)
    # Last layer to ensure initial shape reconstruction.
    up7 = Conv2D(2, (4, 4), dilation_rate=(2, 2), activation='sigmoid', padding='same', data_format='channels_first', kernel_initializer=kernel_initializer)((batch12))
    output = multiply([up7, b])

    paddings = tf.constant([[0,0],[0,0],[0,0],[0,1]])
    output = tf.pad(output, paddings, "CONSTANT")

    m = Model(inputs=input_tensor, outputs=output)

    return m

sample_encoder_layer = apply_unet()

tf.keras.utils.plot_model(sample_encoder_layer, to_file='blstm.png', show_shapes=True)

"""# 训练"""

m_unet = apply_unet()
m_unet.compile(loss='mean_squared_error', optimizer='adam', metrics=[Accuracy()])
m_unet.fit(X[:20,:,:,:], M['vocals'][:20,:,:,:], batch_size=2, epochs=20)

"""# 预测"""

track = [mus[-1]]

X, M = dataset(track)

X_origin = stft(track[0].audio.T, nperseg=4096, noverlap=3072)[-1]

print(X.shape, len(M), M['vocals'].shape, X_origin.shape)

M_predict = m_unet.predict(X)
print(M_predict.shape)

MM_predict = {'vocals': M_predict,
      'drums': M_predict,
      'bass': M_predict,
      'other':M_predict}

def ichop(X_origin, M, time_scale=256, ratio_overlap=0.5): # 输入只一首歌
  channel, frequency, time = X_origin.shape
  newM = {'vocals': np.empty((channel, frequency, time+time_scale), float),
      'drums': np.empty((channel, frequency, time+time_scale), float),
      'bass': np.empty((channel, frequency, time+time_scale), float),
      'other': np.empty((channel, frequency, time+time_scale), float)} # [channel, frequency, time] 
  for name, source in M.items():
    for i in range(source.shape[0]): # 遍历batch
      toInput = np.transpose(source[i,:,:,:], (0, 2, 1))  # (2, 2049, 240)
      if i == 0:
        newM[name][:,:, i:i + time_scale] = toInput
      else:
        
        newM[name][:,:, int(i * time_scale * (1 - ratio_overlap)):int((i * time_scale * (1 - ratio_overlap) + time_scale * ratio_overlap))] = np.true_divide(
              np.add(
              newM[name][:,:, int(i * time_scale * (1 - ratio_overlap)):int(i * time_scale * (1 - ratio_overlap) + time_scale * ratio_overlap)],toInput[:,:, :int(time_scale * ratio_overlap)]), 
              2)
        newM[name][:,:, int(i * time_scale * (1 - ratio_overlap) + time_scale * ratio_overlap):int(
              i * time_scale * (1 - ratio_overlap) + time_scale)] = toInput[:,:,-int(time_scale * (1 - ratio_overlap)):]
    newM[name] = newM[name][:channel, :frequency, :time]
  return newM



newM = ichop(X_origin, MM_predict)
print(newM['vocals'].shape)

def estimateSpectro(X_origin, newM):
  
  # small epsilon to avoid dividing by zero
  eps = np.finfo(np.float).eps
  # compute model as the sum of spectrograms
  model = eps

  for name, source in newM.items():  # 遍历所有声部，求mask中的分母
    model += newM[name]


  # now performs separation
  estimates = {}
  for name, source in newM.items(): # 遍历所有声部，用mask分离出各个声部
    # compute soft mask as the ratio between source spectrogram and total
    Mask = newM[name] / model

    # multiply the mix by the mask
    Yj = Mask * X_origin

    # invert to time domain
    target_estimate = istft(Yj, nperseg=4096, noverlap=3072)[1].T

    # set this as the source estimate
    estimates[name] = target_estimate

  return estimates

estimates = estimateSpectro(X_origin, newM)

from IPython.display import Audio, display

for target, estimate in estimates.items():
  display(Audio(estimate.T, rate=track[0].rate))

display(Audio(track[0].audio.T, rate=track[0].rate))

import museval

track_scores = museval.eval_mus_track(track[0], estimates)
print(track_scores)